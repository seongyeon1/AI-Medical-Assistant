import os
import streamlit as st
import requests
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable
from langchain_openai import ChatOpenAI
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import pandas as pd

from secrets import serviceKey
# Set your API key and endpoint
# serviceKey='Decoding key'
LANGSERVE_ENDPOINT = "http://localhost:8000/llm/c/N4XyA"

# Embedding ì„¤ì •
USE_BGE_EMBEDDING = True

# if not USE_BGE_EMBEDDING:
#     os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"


if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì¹œì ˆí•œ AIì…ë‹ˆë‹¤. ì¦ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë³‘ì„ ì˜ˆì¸¡í•˜ê³ , í•´ë‹¹ ì§ˆë³‘ì˜ ì§„ë£Œê³¼ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- ì˜ˆìƒ ì§ˆë³‘ì— ëŒ€í•œ ì„¤ëª…ê³¼ ëŒ€ì²˜ë°©ì•ˆì— ëŒ€í•´ì„œ ì–˜ê¸°í•´ ì¤ë‹ˆë‹¤. (5ê°œ ì •ë„ê¹Œì§€ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²ƒì„ ì–˜ê¸°í•´ì¤ë‹ˆë‹¤)
- ì„±ë³„, ë‚˜ì´ì— ë”°ë¥¸ ë‹¤ë¥¸ ê²°ê³¼ê°’
- í•œê¸€ë¡œ ì‘ì„±

Question: {question} 
Context: {context} 
Answer:"""

# Read symptoms data
def load_symptoms_data(file_path):
    return pd.read_csv(file_path, delimiter='\t')

symptoms_df = load_symptoms_data('symptoms.txt')

# Streamlit setup
st.set_page_config(page_title="ì§ˆë³‘ ì˜ˆì¸¡ ë° ë³‘ì› ì¶”ì²œ", page_icon="ğŸ¥")
st.title("ì§ˆë³‘ ì˜ˆì¸¡ ë° ë³‘ì› ì¶”ì²œ ì‹œìŠ¤í…œ")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file_path):
    cache_dir = LocalFileStore(f"./.cache/embeddings/{os.path.basename(file_path)}")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\. )", " ", ""],
        length_function=len,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=text_splitter)

    if USE_BGE_EMBEDDING:
        model_name = 'jhgan/ko-sroberta-multitask'
        model_kwargs = {"device": "mps"}
        encode_kwargs = {"normalize_embeddings": True}
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs,
        )
    else:
        embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, embedding=cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

retriever = embed_file('symptoms.txt')
llm = RemoteRunnable(LANGSERVE_ENDPOINT)

print_history()

if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        if retriever is not None:
            prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

            rag_chain = (
                {
                    "context": retriever | format_docs,
                    "question": RunnablePassthrough(),
                }
                | prompt
                | llm
                | StrOutputParser()
            )
            answer = rag_chain.stream(user_input)
            chunks = []
            for chunk in rag_chain.stream(user_input):
                chunks.append(chunk)
            answer = "".join(chunks)
            st.markdown(answer)
            add_history("assistant", "".join(chunks))
        else:
            st.write("ë¬¸ì„œ ì„ë² ë”©ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
